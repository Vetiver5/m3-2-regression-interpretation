{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Diagnostics\n",
    "\n",
    "Here, we'll list some of the other diagnostic statistics so you can read a regression table:\n",
    "\n",
    "## 1. F-Stat\n",
    "\n",
    "The F-test is a test where the null hypothesis that all the coefficients are 0 (eg. your model is no better than the mean). Of interest generally is the **p-value** on that test, which should always be be 0.00 -- otherwise your model has big problems.\n",
    "\n",
    "## 2. Log likelihood\n",
    "\n",
    "The Log likelihood (LL) comes from the model's [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function). Log-likelihood is all your dataset run through the pdf of the likelihood (normal distribution for OLS), and then they are summed together, and then taking the log of this sum.\n",
    "\n",
    "So it's measure of **model loss** in the sense that it's a difference between prediction likelihood and reality.\n",
    "\n",
    "The only real interpretation for log-likelihood is, higher is better.\n",
    "\n",
    "Log-likelihood values cannot be used alone as an index of model fit because they are a function of sample size but can be used to compare the fit of different coefficients.\n",
    "\n",
    "## 3. AIC and BIC\n",
    "\n",
    "The [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) is a **model selection criterion**. It's used to compare different models on the same dataset to compare them.\n",
    "\n",
    "The equation is $AIC = 2k - 2ln(\\hat{L})$ where $k$ is the number of coefficients and $L$ is the model likelihood stat. \n",
    "\n",
    "Generally you will compare models on the same dataset and pick the one with the smallest value, this is to penalize models which could be **overfitting** to the data.\n",
    "\n",
    "The formula for the Bayesian information criterion (BIC) is similar to the formula for AIC, but with a different penalty for the number of parameters. With AIC the penalty is $2k$, whereas with BIC the penalty is $ln(n)k$. \n",
    "\n",
    "## 4. Durbin Watson\n",
    "\n",
    "The [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) test checks if the errors in your model have **autocorrelation** (which would imply heteroscedasticity). \n",
    "\n",
    "So it's a homoscedasticity test.\n",
    "\n",
    "## 5. Skew\n",
    "\n",
    "Skew is a statistic check for equality of dispersion in your model's error term. A distribution can have right or left-skew, but either way this disperses away from normality of errors, invalidating coefficients and standard errors.\n",
    "\n",
    "## 6. Omnibus\n",
    "\n",
    "A test of the skewness and kurtosis of the residual \n",
    "\n",
    "We hope to see a value close to zero which would indicate a normal distribution.\n",
    "\n",
    "The **Prob (Omnibus)** performs a statistical test indicating the probability that the residuals are normally distributed. We hope to see something close to 1 here. \n",
    "\n",
    "## 7. Kurtosis\n",
    "\n",
    "A measure of \"peakiness\", or curvature of the data. Higher peaks lead to greater Kurtosis. Greater Kurtosis can be interpreted as a tighter clustering of residuals around zero, implying a better model with few outliers.\n",
    "\n",
    "# 8. Jarque-Bera (JB)\n",
    "\n",
    "Like the Omnibus test in that it tests both skew and kurtosis. We hope to see in this test a confirmation of the Omnibus test.\n",
    "\n",
    "# 9. Condition Number\n",
    "\n",
    "This test measures the sensitivity of a function's output as compared to its input.\n",
    "\n",
    "When we have severe multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something below 30.\n",
    "\n",
    "When the $X^-2$ matrix blows up, the condition number will raise a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}